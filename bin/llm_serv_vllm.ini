# file: llm_serv_vllm.ini
# date: 2025-02-06


CURR_DIR=$(pwd)
WORKSPACE="./_llm_serv_vllm"
PYTHON=$(which python3)
PORT=6789
VLLM_VERSION="0.8.5.post1"
HF_TOKEN=""
MODEL="unsloth/Llama-3.1-8B-Instruct-unsloth-bnb-4bit"
TOKENIZER="unsloth/Llama-3.1-8B-Instruct-unsloth-bnb-4bit"
CUDA_VISIBLE_DEVICES=0
MAX_MODEL_LEN=20000
TENSOR_PARALLEL_SIZE=1
GPU_MEM_UTILIZATION=0.5
DTYPE=bfloat16
VLLM_ALLOW_LONG_MAX_MODEL_LEN=1
ADAPTER_NAME="info-extraction"
MAX_LORA_RANK=128
ADAPTER_CKPT=""
ADAPTER_BASEMODEL="unsloth/Llama-3.1-8B-Instruct-unsloth-bnb-4bit"


