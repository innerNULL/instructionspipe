{
  "hf": {
    "token": ""
  },
  "wandb": {
    "project": "instructionspipe-dev",
    "key": ""
  },
  "model": {
    "model_name_or_path": "meta-llama/Llama-3.2-3B-Instruct",
    "tokenizer_name_or_path": "meta-llama/Llama-3.2-3B-Instruct",
    "quantization": true
  },
  "peft": {
    "type": "lora",
    "lora_rank": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.1,
    "target_modules": ["q_proj", "v_proj", "v_proj"],
    "use_rslora": true
  },
  "quantization": {
    "load_in_4bit": true, 
    "bnb_4bit_quant_type": "nf4"
  },
  "data": {
    "train_data_path": "./demo_data/training/train_chatml.jsonl",
    "val_data_path": "./demo_data/training/val_chatml.jsonl",
    "train_size": 10000,
    "val_size": 30,
    "chatml_col": "msgs"
  },
  "deepspeed": {
    "train_batch_size": "auto",
    "zero_optimization": {
      "stage": 2
    },
    "fp16": {
        "enabled": true
    },
    "optimizer": {
      "type":  "AdamW",
      "params": {
        "lr": 0.0001, 
        "betas": [0.9, 0.999], 
        "eps": 1e-8 
      }
    }
  },
  "train": {
    "out_dir": "./_sft_dev",
    "learning_rate": 0.0001,
    "num_epochs": 10,
    "per_device_train_batch_size": 2,
    "per_device_eval_batch_size": 2,
    "max_length": 1024,
    "gradient_accumulation_steps": 4
  }
}
