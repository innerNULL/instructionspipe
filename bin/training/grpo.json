{
  "hf": {
    "token": ""
  },
  "wandb": {
    "project": "instructionspipe-sft-grpo-dev",
    "key": ""
  },
  "model": {
    "model_name_or_path": "meta-llama/Llama-3.2-3B-Instruct",
    "tokenizer_name_or_path": "meta-llama/Llama-3.2-3B-Instruct",
    "quantization": true
  },
  "peft": {
    "type": "lora",
    "lora_rank": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.1
  },
  "data": {
    "train_data_path": "./demo_data/training/train_chatml.jsonl",
    "val_data_path": "./demo_data/training/val_chatml.jsonl",
    "train_size": 10000,
    "val_size": 30,
    "chatml_col": "msgs"
  },
  "train": {
    "out_dir": "./_sft_dev",
    "learning_rate": 0.0001,
    "num_epochs": 10,
    "per_device_train_batch_size": 2,
    "per_device_eval_batch_size": 2,
    "max_length": 1024,
    "gradient_accumulation_steps": 4
  },
  "reward": {
    "plugin_path": "./bin/training/grpo_plugin.py",
    "llm_factuality": "unsloth/gemma-3-27b-it-bnb-4bit",
    "msg_idx_instruction": 0, 
    "msg_idx_in_text": 1, 
    "llms": [
      {
        "model": "gpt-4o-mini",
        "model_provider": "azure_openai",
        "azure_endpoint": "tbd",
        "openai_api_version": "2024-05-01-preview",
        "openai_api_key": "tbd",
        "max_retries": 3,
        "timeout": 300,
        "temperature": 0.01,
        "max_tokens": 1024
      },
      {
        "model": "unsloth/gemma-3-27b-it-bnb-4bit",
        "model_provider": "openai",
        "base_url": "",
        "api_key": "null",
        "max_retries": 3,          
        "timeout": 300,    
        "temperature": 0.1,   
        "max_tokens": 1024          
      }
    ]
  }
}
