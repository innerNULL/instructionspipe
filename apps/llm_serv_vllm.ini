# file: llm_serv_vllm.ini
# date: 2025-02-06


CURR_DIR=$(pwd)
WORKSPACE="./_llm_serv_vllm"
PYTHON=$(which python3)
PORT=6789
VLLM_VERSION="0.8.5.post1"
HF_TOKEN=""
MODEL="unsloth/Llama-3.2-3B-Instruct-unsloth-bnb-4bit"
TOKENIZER="unsloth/Llama-3.2-3B-Instruct-unsloth-bnb-4bit"
CUDA_VISIBLE_DEVICES=5
MAX_MODEL_LEN=10000
MAX_NEW_TOKENS=1024
TENSOR_PARALLEL_SIZE=1
PIPELINE_PARALLEL_SIZE=1
GPU_MEM_UTILIZATION=0.4
DTYPE=bfloat16
VLLM_ALLOW_LONG_MAX_MODEL_LEN=1
SERVED_MODEL_NAME="dev-llm"
BITSANDBYTES_QUANTIZATION="true"
ROPE_CONF="{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":32768}"
ROPE_THETA=10000
ADAPTER_NAME="your-adapter-name-or-path"
MAX_LORA_RANK=128
ADAPTER_CKPT=""
ADAPTER_BASEMODEL="unsloth/Llama-3.2-3B-Instruct-unsloth-bnb-4bit"


